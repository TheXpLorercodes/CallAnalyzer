# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mp3gwUJ9hNpv50hkw1HsaQ-dsgq8MJmH
"""

# Call Quality Analyzer for Sales Calls
# Optimized for Google Colab Free Tier - Processing Time: <30 seconds

import os
import re
import time
import warnings
warnings.filterwarnings('ignore')

# Install required packages
print("Installing required packages...")
!pip install -q yt-dlp openai-whisper torch torchaudio vaderSentiment librosa
!pip install -q --upgrade transformers

import torch
import librosa
import whisper
import yt_dlp
import numpy as np
import pandas as pd
from datetime import timedelta
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import matplotlib.pyplot as plt
import seaborn as sns

class CallQualityAnalyzer:
    def __init__(self):
        """Initialize the Call Quality Analyzer with optimized models for Colab free tier"""
        print("ğŸ”§ Initializing Call Quality Analyzer...")

        try:
            # Use smallest Whisper model for speed - with error handling
            print("ğŸ“¥ Loading Whisper model...")
            self.whisper_model = whisper.load_model("tiny")
            print("âœ“ Whisper model loaded (tiny - optimized for speed)")
        except Exception as e:
            print(f"âš ï¸ Whisper loading warning: {e}")
            print("ğŸ”„ Trying alternative Whisper initialization...")
            try:
                # Alternative initialization
                self.whisper_model = whisper.load_model("tiny", device="cpu")
                print("âœ“ Whisper model loaded with CPU fallback")
            except Exception as e2:
                print(f"âŒ Whisper initialization failed: {e2}")
                self.whisper_model = None

        # Initialize sentiment analyzer
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        print("âœ“ Sentiment analyzer ready")

        # Initialize data storage
        self.audio_file = None
        self.transcript_data = []

    def download_audio(self, youtube_url, max_duration=120):
        """Download audio from YouTube with optimization for processing speed"""
        print(f"ğŸµ Downloading audio from: {youtube_url}")

        # Configure yt-dlp for fastest download
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': 'call_audio.%(ext)s',
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'wav',
                'preferredquality': '16',  # Lower quality for speed
            }],
            'postprocessor_args': [
                '-ar', '16000',  # 16kHz sample rate
                '-ac', '1',      # Mono audio
                f'-t', str(max_duration)  # Limit duration for free tier
            ],
        }

        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([youtube_url])

            # Find the downloaded file
            for file in os.listdir('.'):
                if file.startswith('call_audio') and file.endswith('.wav'):
                    self.audio_file = file
                    break

            if self.audio_file:
                print(f"âœ“ Audio downloaded: {self.audio_file}")
                # Check file size
                file_size = os.path.getsize(self.audio_file) / (1024*1024)  # MB
                print(f"ğŸ“ File size: {file_size:.1f} MB")
                return True
            else:
                print("âŒ Failed to find downloaded audio file")
                return False

        except Exception as e:
            print(f"âŒ Download failed: {str(e)}")
            return False

    def simple_speaker_diarization(self, audio_path):
        """Simple speaker diarization using audio features and silence detection"""
        print("ğŸ¯ Performing speaker diarization...")

        try:
            # Load audio
            y, sr = librosa.load(audio_path, sr=16000)

            # Simple voice activity detection using energy
            frame_length = int(0.5 * sr)  # 0.5 second frames
            hop_length = int(0.25 * sr)   # 0.25 second overlap

            # Compute RMS energy
            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

            # Detect speech segments (simple threshold-based)
            threshold = np.percentile(rms, 30)  # Dynamic threshold
            speech_frames = rms > threshold

            # Convert frame indices to time segments
            segments = []
            current_start = None

            for i, is_speech in enumerate(speech_frames):
                time_pos = i * hop_length / sr

                if is_speech and current_start is None:
                    current_start = time_pos
                elif not is_speech and current_start is not None:
                    if time_pos - current_start > 0.5:  # Minimum 0.5 second segment
                        segments.append((current_start, time_pos))
                    current_start = None

            # Close final segment if needed
            if current_start is not None:
                segments.append((current_start, len(y) / sr))

            print(f"âœ“ Found {len(segments)} speech segments")
            return segments

        except Exception as e:
            print(f"âš ï¸ Speaker diarization warning: {e}")
            return []

    def transcribe_with_whisper(self):
        """Transcribe audio using Whisper (compatible version)"""
        print("ğŸ¤ Transcribing audio with Whisper...")

        if not self.audio_file:
            print("âŒ No audio file available")
            return False

        if not self.whisper_model:
            print("âŒ Whisper model not initialized")
            return False

        try:
            # Transcribe audio - compatible method
            print("ğŸ”„ Processing audio file...")
            result = self.whisper_model.transcribe(self.audio_file, language='en', fp16=False)

            # Get segments from Whisper
            segments = result.get('segments', [])

            if not segments:
                # Fallback: create segments from the full text
                print("ğŸ”„ Creating segments from full transcript...")
                full_text = result.get('text', '')

                if not full_text.strip():
                    print("âŒ No transcription text found")
                    return False

                # Split into sentences
                sentences = re.split(r'[.!?]+', full_text)
                sentences = [s.strip() for s in sentences if s.strip()]

                # Create artificial segments with estimated timing
                audio_duration = librosa.get_duration(filename=self.audio_file)
                segment_duration = audio_duration / max(len(sentences), 1)

                segments = []
                for i, sentence in enumerate(sentences):
                    segments.append({
                        'start': i * segment_duration,
                        'end': (i + 1) * segment_duration,
                        'text': sentence
                    })

            print(f"âœ“ Found {len(segments)} transcript segments")

            # Simple speaker assignment using conversation patterns
            current_speaker = "Speaker_1"
            last_end_time = 0

            for i, segment in enumerate(segments):
                start_time = segment['start']
                end_time = segment['end']
                text = segment['text'].strip()

                # Speaker switching logic
                pause_duration = start_time - last_end_time

                # Switch speaker based on:
                # 1. Long pauses (>2 seconds)
                # 2. Short responses (likely acknowledgments)
                # 3. Question patterns
                if i > 0:
                    if pause_duration > 2.0:
                        current_speaker = "Speaker_2" if current_speaker == "Speaker_1" else "Speaker_1"
                    elif len(text.split()) < 3 and any(word in text.lower() for word in ['yes', 'no', 'okay', 'sure', 'right']):
                        current_speaker = "Speaker_2" if current_speaker == "Speaker_1" else "Speaker_1"

                self.transcript_data.append({
                    'speaker': current_speaker,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': end_time - start_time,
                    'text': text
                })

                last_end_time = end_time

            print(f"âœ“ Transcription complete: {len(self.transcript_data)} segments")

            # Display sample transcript
            print("\nğŸ“ Sample transcript:")
            for i, segment in enumerate(self.transcript_data[:4]):
                print(f"   {segment['speaker']} ({segment['start_time']:.1f}s): {segment['text'][:80]}...")

            return True

        except Exception as e:
            print(f"âŒ Transcription failed: {str(e)}")
            print("ğŸ’¡ This might be due to audio format or model compatibility issues")
            return False

    def calculate_talk_time_ratio(self):
        """Calculate talk time ratio for each speaker"""
        print("ğŸ“Š Calculating talk time ratios...")

        speaker_times = {}
        total_time = 0

        for segment in self.transcript_data:
            speaker = segment['speaker']
            duration = segment['duration']

            if speaker not in speaker_times:
                speaker_times[speaker] = 0

            speaker_times[speaker] += duration
            total_time += duration

        # Calculate percentages
        talk_ratios = {}
        for speaker, time in speaker_times.items():
            talk_ratios[speaker] = (time / total_time) * 100 if total_time > 0 else 0

        print(f"âœ“ Talk time calculated")
        return talk_ratios

    def count_questions(self):
        """Count questions asked by each speaker"""
        print("â“ Counting questions...")

        # Multiple question detection patterns for robustness
        question_patterns = [
            r'\?',  # Direct question marks
            r'\b(what|how|when|where|why|who|which)\b[^.]*\??',  # Wh-questions
            r'\b(can|could|would|will|do|does|did|is|are|was|were)\s+you\b[^.]*\??',  # Yes/no questions
            r'\b(tell me|show me|explain)\b[^.]*\??',  # Imperative questions
        ]

        speaker_questions = {}
        total_questions = 0

        for segment in self.transcript_data:
            speaker = segment['speaker']
            text = segment['text']

            if speaker not in speaker_questions:
                speaker_questions[speaker] = 0

            # Count questions using patterns
            question_count = 0
            for pattern in question_patterns:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                question_count += matches

            # Remove duplicates (same sentence matching multiple patterns)
            if question_count > 0:
                # Simple deduplication: max 1 question per short segment
                question_count = min(question_count, max(1, len(text.split()) // 10))

            speaker_questions[speaker] += question_count
            total_questions += question_count

        print(f"âœ“ Questions counted: {total_questions} total")
        return speaker_questions, total_questions

    def find_longest_monologue(self):
        """Find the longest continuous speech by one speaker"""
        print("ğŸ—£ï¸ Finding longest monologue...")

        if not self.transcript_data:
            return None, 0

        max_duration = 0
        longest_speaker = None
        current_speaker = None
        current_duration = 0

        for segment in self.transcript_data:
            if segment['speaker'] != current_speaker:
                # Speaker changed - check if current monologue is longest
                if current_duration > max_duration:
                    max_duration = current_duration
                    longest_speaker = current_speaker

                # Start new monologue
                current_speaker = segment['speaker']
                current_duration = segment['duration']
            else:
                # Same speaker continuing - add to current monologue
                current_duration += segment['duration']

        # Check final monologue
        if current_duration > max_duration:
            max_duration = current_duration
            longest_speaker = current_speaker

        print(f"âœ“ Longest monologue: {max_duration:.1f}s by {longest_speaker}")
        return longest_speaker, max_duration

    def analyze_sentiment(self):
        """Analyze overall call sentiment using VADER"""
        print("ğŸ˜Š Analyzing call sentiment...")

        # Combine all transcript text
        all_text = " ".join([segment['text'] for segment in self.transcript_data])

        if not all_text.strip():
            print("âš ï¸ No text available for sentiment analysis")
            return "Neutral", {'pos': 0, 'neu': 1, 'neg': 0, 'compound': 0}

        # Get sentiment scores
        scores = self.sentiment_analyzer.polarity_scores(all_text)

        # Determine overall sentiment
        if scores['compound'] >= 0.05:
            sentiment = "Positive"
        elif scores['compound'] <= -0.05:
            sentiment = "Negative"
        else:
            sentiment = "Neutral"

        print(f"âœ“ Sentiment: {sentiment} (compound: {scores['compound']:.3f})")
        return sentiment, scores

    def identify_sales_rep_vs_customer(self):
        """Identify which speaker is likely the sales rep vs customer"""
        print("ğŸ•µï¸ Identifying sales rep vs customer...")

        speaker_stats = {}

        # Analyze speaking patterns for each speaker
        for segment in self.transcript_data:
            speaker = segment['speaker']
            text = segment['text'].lower()

            if speaker not in speaker_stats:
                speaker_stats[speaker] = {
                    'total_words': 0,
                    'questions': 0,
                    'sales_keywords': 0,
                    'customer_keywords': 0,
                    'segments': 0,
                    'avg_segment_length': 0
                }

            words = text.split()
            speaker_stats[speaker]['total_words'] += len(words)
            speaker_stats[speaker]['segments'] += 1

            # Count questions
            if '?' in text:
                speaker_stats[speaker]['questions'] += text.count('?')

            # Sales rep indicators (people who talk about products/services)
            sales_keywords = ['offer', 'product', 'service', 'price', 'cost', 'deal', 'discount',
                            'features', 'benefits', 'package', 'solution', 'recommend', 'provide',
                            'help you', 'we can', 'our company', 'let me', 'i can']
            for keyword in sales_keywords:
                if keyword in text:
                    speaker_stats[speaker]['sales_keywords'] += 1

            # Customer indicators (people expressing needs/concerns)
            customer_keywords = ['need', 'looking for', 'interested', 'budget', 'thinking',
                               'consider', 'compare', 'concern', 'worry', 'issue', 'problem',
                               'want', 'require', 'hoping', 'wondering']
            for keyword in customer_keywords:
                if keyword in text:
                    speaker_stats[speaker]['customer_keywords'] += 1

        # Calculate average segment length
        for speaker in speaker_stats:
            if speaker_stats[speaker]['segments'] > 0:
                speaker_stats[speaker]['avg_segment_length'] = (
                    speaker_stats[speaker]['total_words'] / speaker_stats[speaker]['segments']
                )

        # Determine roles based on patterns
        role_assignments = {}

        for speaker, stats in speaker_stats.items():
            # Sales reps typically:
            # - Use more sales keywords
            # - Have longer segments (explaining products)
            # - Ask fewer questions
            sales_score = (
                stats['sales_keywords'] * 3 +  # Heavy weight on sales language
                (stats['avg_segment_length'] > 15) * 2 +  # Longer explanations
                (stats['questions'] < 2) * 1  # Fewer questions
            )

            # Customers typically:
            # - Express needs and concerns
            # - Ask more questions
            # - Have shorter, more reactive responses
            customer_score = (
                stats['customer_keywords'] * 3 +
                stats['questions'] * 2 +  # More questions
                (stats['avg_segment_length'] < 10) * 1  # Shorter responses
            )

            if sales_score > customer_score:
                role_assignments[speaker] = "Sales Rep"
            else:
                role_assignments[speaker] = "Customer"

        print(f"âœ“ Role identification complete")
        return role_assignments

    def generate_actionable_insight(self, talk_ratios, questions, sentiment, roles):
        """Generate one actionable insight based on the call analysis"""
        print("ğŸ’¡ Generating actionable insight...")

        insights = []

        # Analyze talk time ratio for sales effectiveness
        if roles:
            sales_speakers = [k for k, v in roles.items() if v == "Sales Rep"]
            customer_speakers = [k for k, v in roles.items() if v == "Customer"]

            if sales_speakers:
                sales_ratio = talk_ratios.get(sales_speakers[0], 0)
                if sales_ratio > 75:
                    insights.append("ğŸ¯ Sales rep is dominating the conversation (>75% talk time). Practice the 70/30 rule: let customers talk 70% of the time to better understand their needs.")
                elif sales_ratio < 25:
                    insights.append("ğŸ¯ Sales rep needs to be more assertive. Take control of the conversation flow and present solutions more confidently.")

        # Analyze question effectiveness
        total_questions = sum(questions.values())
        if total_questions < 3:
            insights.append("ğŸ¯ Too few discovery questions asked (<3). Implement BANT qualification: Budget, Authority, Need, Timeline questions to better qualify prospects.")
        elif total_questions > 15:
            insights.append("ğŸ¯ Too many questions without solutions. After discovery, transition to presenting relevant solutions that address identified pain points.")

        # Sentiment-based coaching
        if sentiment == "Negative":
            insights.append("ğŸ¯ Negative call sentiment detected. Focus on active listening, empathy, and addressing objections before presenting solutions.")
        elif sentiment == "Neutral":
            insights.append("ğŸ¯ Neutral sentiment suggests low engagement. Build more rapport, use storytelling, and create emotional connection to the value proposition.")

        # Default insight for positive calls
        if not insights and sentiment == "Positive":
            insights.append("ğŸ¯ Positive call momentum! Capitalize by asking for next steps: schedule follow-up, request referrals, or close for commitment.")

        # Fallback insight
        if not insights:
            insights.append("ğŸ¯ Maintain current approach while focusing on discovering customer pain points and presenting targeted solutions.")

        # Return the most critical insight
        selected_insight = insights[0] if insights else "Continue monitoring call quality metrics and customer engagement."
        print(f"âœ“ Key insight identified")

        return selected_insight

    def create_visualization(self, talk_ratios, sentiment_scores):
        """Create visualization of call metrics"""
        print("ğŸ“Š Creating visualizations...")

        try:
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

            # 1. Talk time ratio pie chart
            if talk_ratios:
                speakers = list(talk_ratios.keys())
                ratios = list(talk_ratios.values())
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

                wedges, texts, autotexts = ax1.pie(ratios, labels=speakers, autopct='%1.1f%%',
                                                  colors=colors[:len(speakers)], startangle=90)
                ax1.set_title('Talk Time Distribution', fontsize=14, fontweight='bold')

            # 2. Sentiment breakdown
            sentiment_labels = ['Positive', 'Neutral', 'Negative']
            sentiment_values = [sentiment_scores['pos'], sentiment_scores['neu'], sentiment_scores['neg']]

            bars = ax2.bar(sentiment_labels, sentiment_values, color=['#2ECC71', '#F39C12', '#E74C3C'])
            ax2.set_title('Sentiment Analysis', fontsize=14, fontweight='bold')
            ax2.set_ylabel('Sentiment Score')
            ax2.set_ylim(0, 1)

            # Add value labels on bars
            for bar, value in zip(bars, sentiment_values):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')

            # 3. Speaking timeline
            if self.transcript_data:
                timeline_data = []
                for segment in self.transcript_data:
                    timeline_data.append({
                        'start': segment['start_time'],
                        'speaker': segment['speaker'],
                        'duration': segment['duration']
                    })

                df = pd.DataFrame(timeline_data)
                speakers = df['speaker'].unique()
                colors_timeline = ['#FF6B6B', '#4ECDC4']

                for i, speaker in enumerate(speakers):
                    speaker_data = df[df['speaker'] == speaker]
                    ax3.scatter(speaker_data['start'], [i] * len(speaker_data),
                               s=speaker_data['duration']*20, alpha=0.7,
                               label=speaker, color=colors_timeline[i % len(colors_timeline)])

                ax3.set_xlabel('Time (seconds)')
                ax3.set_title('Speaking Timeline', fontsize=14, fontweight='bold')
                ax3.set_yticks(range(len(speakers)))
                ax3.set_yticklabels(speakers)
                ax3.legend()

            # 4. Word count and segment analysis
            speaker_stats = {}
            for segment in self.transcript_data:
                speaker = segment['speaker']
                word_count = len(segment['text'].split())

                if speaker not in speaker_stats:
                    speaker_stats[speaker] = {'words': 0, 'segments': 0}

                speaker_stats[speaker]['words'] += word_count
                speaker_stats[speaker]['segments'] += 1

            if speaker_stats:
                speakers = list(speaker_stats.keys())
                word_counts = [speaker_stats[s]['words'] for s in speakers]

                bars = ax4.bar(speakers, word_counts, color=colors[:len(speakers)])
                ax4.set_title('Total Words Spoken', fontsize=14, fontweight='bold')
                ax4.set_ylabel('Word Count')

                # Add value labels
                for bar, count in zip(bars, word_counts):
                    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(word_counts)*0.01,
                            f'{count}', ha='center', va='bottom')

            plt.suptitle('Call Quality Analysis Dashboard', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"âš ï¸ Visualization warning: {e}")
            print("ğŸ“Š Analysis complete - skipping charts due to display issues")

    def analyze_call(self, youtube_url):
        """Main analysis function that processes the entire call"""
        start_time = time.time()
        print("ğŸš€ Starting Call Quality Analysis...")
        print("=" * 60)

        # Step 1: Download audio
        if not self.download_audio(youtube_url):
            return None

        # Step 2: Transcribe audio
        if not self.transcribe_with_whisper():
            return None

        # Step 3: Analyze metrics
        print("\nğŸ“ˆ Analyzing call metrics...")

        # 1. Talk-time ratio
        talk_ratios = self.calculate_talk_time_ratio()

        # 2. Questions count
        speaker_questions, total_questions = self.count_questions()

        # 3. Longest monologue
        longest_speaker, longest_duration = self.find_longest_monologue()

        # 4. Sentiment analysis
        sentiment, sentiment_scores = self.analyze_sentiment()

        # 5. Identify roles (bonus)
        roles = self.identify_sales_rep_vs_customer()

        # 6. Generate actionable insight
        insight = self.generate_actionable_insight(talk_ratios, speaker_questions, sentiment, roles)

        # Calculate processing time
        processing_time = time.time() - start_time

        # Compile results
        results = {
            'talk_time_ratio': talk_ratios,
            'questions_asked': total_questions,
            'questions_by_speaker': speaker_questions,
            'longest_monologue': {
                'speaker': longest_speaker,
                'duration_seconds': longest_duration
            },
            'sentiment': sentiment,
            'sentiment_scores': sentiment_scores,
            'speaker_roles': roles,
            'actionable_insight': insight,
            'processing_time': processing_time,
            'total_segments': len(self.transcript_data)
        }

        self.display_results(results)
        self.create_visualization(talk_ratios, sentiment_scores)

        return results

    def display_results(self, results):
        """Display formatted results"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ CALL QUALITY ANALYSIS RESULTS")
        print("=" * 60)

        print(f"\nâ±ï¸  Processing Time: {results['processing_time']:.1f} seconds")
        print(f"ğŸ“Š Total Segments Analyzed: {results['total_segments']}")

        print(f"\nğŸ¯ 1. TALK-TIME RATIO:")
        for speaker, ratio in results['talk_time_ratio'].items():
            role = results['speaker_roles'].get(speaker, 'Unknown')
            print(f"   â€¢ {speaker} ({role}): {ratio:.1f}%")

        print(f"\nâ“ 2. QUESTIONS ASKED: {results['questions_asked']} total")
        for speaker, count in results['questions_by_speaker'].items():
            role = results['speaker_roles'].get(speaker, 'Unknown')
            print(f"   â€¢ {speaker} ({role}): {count} questions")

        print(f"\nğŸ—£ï¸  3. LONGEST MONOLOGUE:")
        monologue = results['longest_monologue']
        if monologue['speaker']:
            role = results['speaker_roles'].get(monologue['speaker'], 'Unknown')
            print(f"   â€¢ {monologue['speaker']} ({role}): {monologue['duration_seconds']:.1f} seconds")
        else:
            print("   â€¢ No significant monologues detected")

        print(f"\nğŸ˜Š 4. CALL SENTIMENT: {results['sentiment']}")
        scores = results['sentiment_scores']
        print(f"   â€¢ Positive: {scores['pos']:.3f}")
        print(f"   â€¢ Neutral: {scores['neu']:.3f}")
        print(f"   â€¢ Negative: {scores['neg']:.3f}")
        print(f"   â€¢ Overall Score: {scores['compound']:.3f}")

        print(f"\nğŸ­ 5. SPEAKER IDENTIFICATION:")
        for speaker, role in results['speaker_roles'].items():
            talk_time = results['talk_time_ratio'].get(speaker, 0)
            questions = results['questions_by_speaker'].get(speaker, 0)
            print(f"   â€¢ {speaker}: {role} ({talk_time:.1f}% talk time, {questions} questions)")

        print(f"\nğŸ’¡ 6. ACTIONABLE INSIGHT:")
        print(f"   {results['actionable_insight']}")

        print("\n" + "=" * 60)
        print("âœ… Analysis Complete!")

# Test and Usage Functions
def test_analyzer():
    """Test function to verify analyzer setup"""
    print("ğŸ§ª Testing Call Quality Analyzer setup...")

    try:
        # Test Whisper
        print("ğŸ”„ Testing Whisper model...")
        model = whisper.load_model("tiny")
        print("âœ“ Whisper model loads successfully")

        # Test sentiment analyzer
        print("ğŸ”„ Testing sentiment analyzer...")
        analyzer = SentimentIntensityAnalyzer()
        test_scores = analyzer.polarity_scores("This is a great sales call!")
        print("âœ“ Sentiment analyzer works")

        # Test audio processing
        print("ğŸ”„ Testing audio libraries...")
        import librosa
        print("âœ“ Audio processing libraries ready")

        print("âœ… All components working! Ready to analyze calls.")
        return True

    except Exception as e:
        print(f"âŒ Setup test failed: {str(e)}")
        print("ğŸ’¡ Try restarting the runtime and running again")
        return False

def main():
    """Main function to run the call quality analyzer"""

    # Test setup first
    print("ğŸ”§ CALL QUALITY ANALYZER INITIALIZATION")
    print("=" * 60)

    if not test_analyzer():
        print("âŒ Setup failed. Please restart runtime and try again")
        return None

    # Initialize analyzer
    print("\nğŸš€ Initializing analyzer...")
    analyzer = CallQualityAnalyzer()

    # Target YouTube URL (sales call example)
    youtube_url = "https://www.youtube.com/watch?v=4ostqJD3Psc"

    print(f"\nğŸ¯ CALL ANALYSIS STARTING")
    print(f"ğŸ“º Target URL: {youtube_url}")
    print(f"âš¡ Optimized for Google Colab Free Tier")
    print(f"ğŸ¯ Target Processing Time: <30 seconds")
    print("=" * 60)

    # Run analysis
    results = analyzer.analyze_call(youtube_url)

    if results:
        print(f"\nğŸ‰ SUCCESS! Analysis completed in {results['processing_time']:.1f} seconds")

        # Clean up audio file
        if analyzer.audio_file and os.path.exists(analyzer.audio_file):
            os.remove(analyzer.audio_file)
            print("ğŸ§¹ Temporary files cleaned up")

        return results
    else:
        print("\nâŒ Analysis failed! Check error messages above.")
        return None

# Run the analyzer
if __name__ == "__main__":
    results = main()